# üöß Development Status: Work in Progress

**Warning:** This package is under active development and not yet stable. APIs and features may change without notice. Use at your own risk in production environments.

# camh/laravel-ollama

Laravel client cho **Ollama** (local LLM): generate, chat, embeddings + PII masking + JSON guardrails.

## C√†i ƒë·∫∑t
```bash
composer require camh/laravel-ollama
php artisan vendor:publish --tag=config --provider="Camh\\Ollama\\OllamaServiceProvider"
```

## .env
```
OLLAMA_BASE=http://127.0.0.1:11434
OLLAMA_GEN_MODEL=llama3.1
OLLAMA_EMBED_MODEL=nomic-embed-text
```

## V√≠ d·ª•
```php
use Camh\\Ollama\\Facades\\Ollama;

$text = Ollama::generate('Xin ch√†o!', ['options'=>['temperature'=>0.2]]);
$reply = Ollama::chat([['role'=>'user','content'=>'T·∫°o migration users']], []);
$vec   = Ollama::embed('Xin ch√†o th·∫ø gi·ªõi');
```

## Artisan
```bash
php artisan ollama:tags
php artisan ollama:pull llama3.1
php artisan ollama:chat "N√≥i xin ch√†o b·∫±ng 1 c√¢u"
```

## Testing
```bash
composer test
```
